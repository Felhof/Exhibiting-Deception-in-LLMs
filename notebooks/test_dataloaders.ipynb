{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17 13:57:57,570] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "from data.create_qa_dataloaders import create_qa_dataloaders\n",
    "from utils import set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to your W&B account on console before running this\n",
    "# https://docs.wandb.ai/quickstart#2-log-in-to-wb\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up models & tokeniser, loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m set_seed(\u001b[39m62\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m#model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed(62)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "judge = GPT2LMHeadModel.from_pretrained('gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39madd_special_tokens({\u001b[39m\"\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m      2\u001b[0m judge\u001b[39m.\u001b[39mresize_token_embeddings(\u001b[39mlen\u001b[39m(tokenizer))\n\u001b[1;32m      4\u001b[0m train_prop \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "judge.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "train_prop = 0.8\n",
    "batch_size = 16\n",
    "shuffle = True\n",
    "\n",
    "train_loader, test_loader = create_qa_dataloaders('data/processed/TruthfulQA_labeled.csv', tokenizer, train_prop, batch_size, shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW as Harriet\n",
    "\n",
    "\n",
    "lr = 5e-5\n",
    "optimizer = Harriet(judge.parameters(), lr=lr)\n",
    "pad_idx = len(tokenizer) - 1  # Since padding token is last token\n",
    "ce_loss = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "num_epochs = 20\n",
    "\n",
    "log_accuracy_every_batch = 50  # How many steps we compute the accuracy over\n",
    "save_every_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/harriet/Documents/AISHSummer/g5-rhys/wandb/run-20230714_115319-u1xhrau2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA/runs/u1xhrau2' target=\"_blank\">wild-pyramid-2</a></strong> to <a href='https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA' target=\"_blank\">https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA/runs/u1xhrau2' target=\"_blank\">https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA/runs/u1xhrau2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/detecting-and-mitigating-deception/Finetuning-TruthfulQA/runs/u1xhrau2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1464c6160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "config = config={\n",
    "        \"lr\": lr,\n",
    "        \"batch size\": batch_size,\n",
    "        \"epochs\": num_epochs,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Finetuning-TruthfulQA\",\n",
    "    name=None,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [39:02,  9.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m last_logits \u001b[39m=\u001b[39m last_logits\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m ce_loss(last_logits, label)\n\u001b[0;32m---> 23\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/g5-rhys-XxAutN8L/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/g5-rhys-XxAutN8L/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "acc = []\n",
    "yes_idx = tokenizer(\"Yes\").input_ids[0]\n",
    "no_idx = tokenizer(\"No\").input_ids[0]\n",
    "\n",
    "judge.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for train_idx, batch in tqdm(enumerate(train_loader)):\n",
    "        input_ids, attention_mask, label = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = judge(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_logits = output.logits[:, -1:, :]\n",
    "        last_logits = last_logits.squeeze(1)\n",
    "\n",
    "        loss = ce_loss(last_logits, label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\"train/loss\": loss}\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        token_probs = torch.softmax(last_logits, -1)\n",
    "        top_token = torch.argmax(token_probs, dim=1)\n",
    "\n",
    "        batch_acc_yes = torch.sum((label==top_token) * (label==yes_idx))\n",
    "        batch_acc_no = torch.sum((label==top_token) * (label==no_idx))\n",
    "        batch_acc = (batch_acc_yes + batch_acc_no) / batch_size\n",
    "        acc.append(batch_acc)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % log_accuracy_every_batch == 0:\n",
    "            this_acc = sum(acc) / len(acc)\n",
    "            wandb.log({\"train/acc\": this_acc})\n",
    "            acc = []\n",
    "\n",
    "        # Test loop\n",
    "        if global_step % 50 == 0:\n",
    "            judge.eval()\n",
    "            total_test_loss = 0\n",
    "            test_acc = []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    input_ids, attention_mask, label = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    label = label.to(device)\n",
    "\n",
    "                    output = judge(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    last_logits = output.logits[:, -1:, :]  # Double check\n",
    "                    last_logits = last_logits.squeeze(1)\n",
    "\n",
    "                    loss = ce_loss(last_logits, label)\n",
    "                    total_test_loss += loss.item()\n",
    "\n",
    "                    # Accuracy\n",
    "                    token_probs = torch.softmax(last_logits, -1)\n",
    "                    top_token = torch.argmax(token_probs, dim=1)\n",
    "\n",
    "                    batch_acc_yes = torch.sum((label==top_token) * (label==yes_idx))\n",
    "                    batch_acc_no = torch.sum((label==top_token) * (label==no_idx))\n",
    "                    batch_acc = (batch_acc_yes + batch_acc_no) / batch_size\n",
    "                    test_acc.append(batch_acc)\n",
    "\n",
    "            avg_loss = total_test_loss / len(test_loader)\n",
    "            avg_acc = sum(test_acc) / len(test_acc)\n",
    "            metrics = {\"test/loss\": avg_loss, \"test/acc\": avg_acc}\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            judge.train()\n",
    "\n",
    "    if epoch % save_every_epoch == 0:\n",
    "        judge_save_path = \"gpt2-judge-finetuned-epoch{:}.pt\".format(epoch)\n",
    "        torch.save(judge.state_dict(), judge_save_path)\n",
    "        wandb.save(judge_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
